---
title: "SelvarClustMV"
author: "Ho Huu Binh"
date: "`r Sys.Date()`"
toc: true
format:
  html: 
    toc: true
    toc_float: true
    code: true
    code-fold: true
    code-tools: true
  pdf: 
    fontsize: "12"
    toc: true
    number-sections: true
    number-depth: 3
---

## Loading libraries
```{r, warning=F, message=F}
library(MixAll)
library(MASS)
library(mclust)
library(clustvarsel)
library(stats)
library(VarSelLCM)
library(BMA)
library(mlogitBMA)
library(mlogit)
library(foreach)
library(parallel)
library(doParallel)
library(iterators)
library(reshape2)
library(ggplot2)

rm(list=ls())
source("amputation.R")
```

16-11-2024:
+ The code follows a similar logic to the C++ code in terms of using BIC to select variables and assign them to a final set W
+ However, it lacks the iterative refinement that the C++ version provides. 
+ The C++ implementation's iterative, stepwise nature allows for a more dynamic and granular adjustment of the selected variables, whereas the R implementation is based on a one-shot model selection using BMA tools.



## BIC regression
```{r}
compute_bic_reg <- function(x, y, family = "gaussian", full_res = FALSE) {
  # Ensure required packages are loaded
  required_packages <- c("BMA", "nnet", "mlogitBMA")
  for (pkg in required_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      stop(paste("Package", pkg, 
                 "is required but not installed. Please install it."))
    }
  }
  
  x <- as.matrix(x)
  
  if (family != "multinomial") {
    y <- as.vector(unlist(y)) 
  } else {
    y <- as.factor(unlist(y)) 
  }
  
  # Check length
  if (nrow(x) != length(y)) {
    stop(paste("Mismatch in lengths: x has", nrow(x), 
               "rows but y has", length(y), "elements"))
  }
  
  n <- length(y)
  
  # Initialize variables
  selected_vars <- NULL
  bic_value <- NA
  fit <- NULL
  
  if (family != "multinomial") {
    bic_model <- bic.glm(x, y, glm.family = family, 
                         strict = FALSE, nbest = 1)
    
    # Select the best model
    best_model <- bic_model$which[1, ]
    selected_vars <- which(best_model)
    
    # Fit on selected vars
    if (family == "gaussian") {
      fit <- lm(y ~ ., data = data.frame(y = y, 
                                         x = x[, selected_vars, drop = FALSE]))
    } else if (family == "poisson") {
      fit <- glm(y ~ ., data = data.frame(y = y, 
                                          x = x[, selected_vars, drop = FALSE]),
                 family = poisson())
    }
    
    # Compute BIC
    bic_value <- -BIC(fit)
    
  } else {
    data_df <- as.data.frame(cbind(y, x))
    names(data_df)[1] <- "y"
    
    f_mnl <- as.formula(paste("y ~", paste(colnames(x), collapse = " + ")))
    
    # Perform BMA for Multinomial Logistic Regression
    bic_model <- tryCatch({
      bic.mlogit(f = f_mnl, data = data_df, 
                 base.choice = 1, varying = NULL, sep = ".", 
                 approx = TRUE, include.intercepts = TRUE, nbest = 1)
    }, error = function(e) {
      stop("Error in bic.mlogit: ", e$message)
    })
    
    # Extract the selected variables
    selected_models <- bic_model$bic.glm$which
    selected_vars <- which(selected_models)
    
    f_sel <- as.formula(paste("y ~", 
                              paste(colnames(x)[selected_vars], 
                                    collapse = " + ")))
    
    fit <- tryCatch({
      nnet::multinom(f_sel, data = data_df, trace = FALSE)
    }, error = function(e) {
      stop("Error in fitting multinomial model: ", e$message)
    })
    
    # Compute BIC for the multinomial model
    ll <- logLik(fit)
    # Number of parameters: (num preds + intercept) * (num classes - 1)
    p <- (length(selected_vars) + 1) * (length(levels(y)) - 1)
    bic_value <- -(-2 * as.numeric(ll) + log(n) * p)
  }
  
  if (full_res){
    return(list(BIC = bic_value, 
                selected_variables = selected_vars, 
                model = fit))
  }
  else {
    return(BIC = bic_value)
  }
}
```

## Parallel version: No imputed 
```{r}
clvarselgrbkw_mixall <- function(X, G = 2:9, strategy = clusterStrategy(),
                                        samp = FALSE, sampsize = 2000,
                                        BIC.diff = 0, itermax = 100,
                                        verbose = interactive(),
                                        num_cores = parallel::detectCores() - 1)
{
  require(parallel)
  
  # Convert X to a data frame if it's not already
  X <- as.data.frame(X)
  n <- nrow(X)
  d <- ncol(X)
  if(is.null(colnames(X))) 
    colnames(X) <- paste("X", 1:d, sep = "")
  G <- setdiff(G, 1)
  
  # If needed, sample the subset of observations
  if(samp) { sub <- sample(1:n, min(sampsize,n), replace = FALSE) }
  else     { sub <- seq.int(1,n) }

  # Function to determine data type
  get_data_type <- function(col) {
    if (is.numeric(col) && all(col %% 1 == 0, na.rm = TRUE)) {
      return("poisson")
    } else if (is.factor(col) || is.character(col)) {
      return("categorical")
    } else {
      return("gaussian")
    }
  }

  # Prepare data and determine clustering function
  prepare_data_and_cluster_function <- function(data) {
    data_types <- sapply(data, get_data_type)
    unique_types <- unique(data_types)
    
    if (length(unique_types) == 1) {
      cluster_fun <- switch(unique_types,
                          "poisson" = clusterPoisson,
                          "categorical" = clusterCategorical,
                          "gaussian" = clusterDiagGaussian)
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data, models = models[[1]], 
                 cluster_fun = cluster_fun, is_mixed = FALSE))
    } else {
      data_list <- lapply(unique_types, function(type) {
        data[, data_types == type, drop = FALSE]
      })
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data_list, models = models, 
                 cluster_fun = clusterMixedData, 
                 is_mixed = TRUE))
    }
  }

  # Function to fit clustering model
  fit_cluster_model <- function(data, G) {
    tryCatch({
      prepared <- prepare_data_and_cluster_function(data)
      if (prepared$is_mixed) {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                    strategy = strategy, criterion = "BIC", 
                                    models = prepared$models, nbCore = 0)
      } else {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                    strategy = strategy, criterion = "BIC",
                                    models = prepared$models, nbCore = 0)
      }
      return(list(model = model, BIC = -model@criterion, nbCluster = model@nbCluster))
    }, error = function(e) {
      return(list(model = NULL, BIC = -Inf, nbCluster = NA))
    })
  }

  # Initialize cluster for parallel processing
  cl <- makeCluster(num_cores)
  on.exit(stopCluster(cl))
  
  # Load required packages on all worker nodes
  clusterEvalQ(cl, {
    library(MixAll)
    library(BMA)
    library(nnet)
    library(mlogitBMA)
    NULL
  })
  
  # Export necessary functions and objects to the cluster
  clusterExport(cl, c("prepare_data_and_cluster_function", "get_data_type", 
                     "fit_cluster_model", "compute_bic_reg",
                     "strategy"), envir = environment())

  # Initialize variables
  S <- X
  NS <- data.frame(matrix(ncol = 0, nrow = nrow(X)))
  info <- data.frame(Var = character(), BIC = numeric(), BICdiff = numeric(), 
                    Step = character(), Decision = character(), 
                    Model = character(), G = integer(), 
                    stringsAsFactors = FALSE)

  # Initial clustering
  if (verbose) cat("Initialize model\n")
  init_model <- fit_cluster_model(S, G)
  BICS <- init_model$BIC

  criterion <- 1
  iter <- 0

  while((criterion == 1) & (iter < itermax) & (ncol(S) > 1)) {
    iter <- iter + 1
    if(verbose) cat(paste("iter", iter, "\n"))

    # Removing step - Parallel processing
    if(verbose) cat("- removing step\n")
    remove_results <- parLapply(cl, 1:ncol(S), function(i) {
      S_minus_i <- S[, -i, drop = FALSE]
      model_minus_i <- fit_cluster_model(S_minus_i, G)
      
      # Calculate BIC for regression of removed variable on remaining variables
      fully_observed <- complete.cases(S_minus_i)
      S_minus_i_obs <- S_minus_i[fully_observed, , drop = FALSE]
      S_i_obs <- S[fully_observed, i, drop = FALSE]
      BICreg <- compute_bic_reg(S_minus_i_obs, S_i_obs,
                               family = get_data_type(S[, i]))
      
      list(BIC_total = model_minus_i$BIC + BICreg, 
           BICreg = BICreg,
           model = model_minus_i$model, 
           nbCluster = model_minus_i$nbCluster)
    })
    
    BIC_remove <- sapply(remove_results, function(x) x$BIC_total)
    BIC_reg <- sapply(remove_results, function(x) x$BICreg)
    cdiff_remove <- BICS - BIC_remove
    m_remove <- min(cdiff_remove)
    arg_remove <- which.min(cdiff_remove)

    if(m_remove < BIC.diff) {
      # Remove variable
      removed_var <- colnames(S)[arg_remove]
      BICS <- BICS - BIC_reg[arg_remove] - cdiff_remove[arg_remove]
      info <- rbind(info, data.frame(
        Var = removed_var, BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Accepted",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
      NS[[removed_var]] <- S[[removed_var]]
      S <- S[, -arg_remove, drop = FALSE]
    } else {
      info <- rbind(info, data.frame(
        Var = colnames(S)[arg_remove], BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Rejected",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
    }

    # Adding step - Parallel processing
      if(ncol(NS) > 2) {
        if(verbose) cat("+ adding step\n")
        add_results <- parLapply(cl, 1:ncol(NS), function(i) {
          S_plus_i <- cbind(S, NS[, i, drop = FALSE])
          model_plus_i <- fit_cluster_model(S_plus_i, G)
          
          # Calculate BIC for regression of added variable on current variables
          fully_observed <- complete.cases(S)
          S_obs <- S[fully_observed, , drop = FALSE]
          NS_i_obs <- NS[fully_observed, i, drop = FALSE]
          BICreg <- compute_bic_reg(S_obs, NS_i_obs, 
                                   family = get_data_type(S_obs))
          
          list(BIC_total = model_plus_i$BIC,
               BICreg = BICreg,
               model = model_plus_i$model,
               nbCluster = model_plus_i$nbCluster)
        })
  
        BIC_add <- sapply(add_results, function(x) x$BIC_total)
        BIC_reg <- sapply(add_results, function(x) x$BICreg)
        cdiff_add <- BIC_add - (BICS + BIC_reg)
        m_add <- max(cdiff_add)
        arg_add <- which.max(cdiff_add)
  
        if(m_add > BIC.diff) {
          # Add variable to S and update clustering BICS
          added_var <- colnames(NS)[arg_add]
          BICS <- BIC_add[arg_add]
          info <- rbind(info, data.frame(
            Var = added_var, BIC = BICS, BICdiff = m_add,
            Step = "Add", Decision = "Accepted",
            Model = class(add_results[[arg_add]]$model)[1],
            G = add_results[[arg_add]]$nbCluster,
            stringsAsFactors = FALSE
          ))
          S[[added_var]] <- NS[[added_var]]
          NS <- NS[, -arg_add, drop = FALSE]
        } else {
          info <- rbind(info, data.frame(
            Var = colnames(NS)[arg_add], BIC = BIC_add[arg_add], BICdiff = m_add,
            Step = "Add", Decision = "Rejected",
            Model = class(add_results[[arg_add]]$model)[1],
            G = add_results[[arg_add]]$nbCluster,
            stringsAsFactors = FALSE
          ))
        }
      }

    if(verbose) {
      print(info[nrow(info), c("Var", "BICdiff", "Step", "Decision")])
    }

    # Check if the variables in S have changed
    criterion <- if(ncol(S) == 0) 0 else 1
  }

  if(iter >= itermax) 
    warning("Algorithm stopped because maximum number of iterations was reached")

  # Prepare output
  varnames <- colnames(X)
  subset <- if(ncol(S) == 0) NULL else match(colnames(S), varnames)

  # Fit final model to get optimal number of clusters
  final_model <- fit_cluster_model(S, G)
  optimal_G <- final_model$nbCluster

  out <- list(
    variables = varnames,
    subset = subset,
    cluster_model = final_model,
    steps.info = info,
    optimal_G = optimal_G,
    search = "greedy",
    direction = "backward"
  )
  class(out) <- "clustvarsel_mixall"
  return(out)
}
```

# Parallel version: imputed R set 
```{r}
clvarselgrbkw_mixall_imputed <- function(X, 
                                          G = 2:9, 
                                          strategy = clusterStrategy(),
                                          samp = FALSE, 
                                          sampsize = 2000,
                                          BIC.diff = 0, 
                                          itermax = 100,
                                          verbose = interactive(),
                                          num_cores = parallel::detectCores() - 1)
{
  require(parallel)
  
  # Convert X to a data frame if it's not already
  X <- as.data.frame(X)
  n <- nrow(X)
  d <- ncol(X)
  if(is.null(colnames(X))) 
    colnames(X) <- paste("X", 1:d, sep = "")
  G <- setdiff(G, 1)
  
  # If needed, sample the subset of observations
  if(samp) { sub <- sample(1:n, min(sampsize,n), replace = FALSE) }
  else     { sub <- seq.int(1,n) }
  
  # Function to determine data type
  get_data_type <- function(col) {
    if (is.numeric(col) && all(col %% 1 == 0, na.rm = TRUE)) {
      return("poisson")
    } else if (is.factor(col) || is.character(col)) {
      return("categorical")
    } else {
      return("gaussian")
    }
  }
  
  # Prepare data and determine clustering function
  prepare_data_and_cluster_function <- function(data) {
    data_types <- sapply(data, get_data_type)
    unique_types <- unique(data_types)
    
    if (length(unique_types) == 1) {
      cluster_fun <- switch(unique_types,
                            "poisson" = clusterPoisson,
                            "categorical" = clusterCategorical,
                            "gaussian" = clusterDiagGaussian)
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data, models = models[[1]], 
                  cluster_fun = cluster_fun, is_mixed = FALSE))
    } else {
      data_list <- lapply(unique_types, function(type) {
        data[, data_types == type, drop = FALSE]
      })
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data_list, models = models, 
                  cluster_fun = clusterMixedData, 
                  is_mixed = TRUE))
    }
  }
  
  # Function to fit clustering model
  fit_cluster_model <- function(data, G) {
    tryCatch({
      prepared <- prepare_data_and_cluster_function(data)
      if (prepared$is_mixed) {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                      strategy = strategy, criterion = "BIC", 
                                      models = prepared$models, nbCore = 0)
      } else {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                      strategy = strategy, criterion = "BIC",
                                      models = prepared$models, nbCore = 0)
      }
      return(list(model = model, BIC = -model@criterion, nbCluster = model@nbCluster))
    }, error = function(e) {
      return(list(model = NULL, BIC = -Inf, nbCluster = NA))
    })
  }
  
  # Initialize cluster for parallel processing
  cl <- makeCluster(num_cores)
  on.exit(stopCluster(cl))
  
  # Load required packages on all worker nodes
  clusterEvalQ(cl, {
    library(MixAll)
    library(BMA)
    library(nnet)
    library(mlogitBMA)
    NULL
  })
  
  # Initialize variables
  S <- X
  NS <- data.frame(matrix(ncol = 0, nrow = nrow(X)))
  info <- data.frame(Var = character(), BIC = numeric(), BICdiff = numeric(), 
                     Step = character(), Decision = character(), 
                     Model = character(), G = integer(), 
                     stringsAsFactors = FALSE)
  
  # Initial clustering
  if (verbose) cat("Initialize model\n")
  init_model <- fit_cluster_model(S, G)
  BICS <- init_model$BIC
  current_model <- init_model$model
  
  # Get initial imputed values
  imputed_values_current <- as.data.frame(missingValues(current_model))
  if (!is.null(imputed_values_current) && length(imputed_values_current) > 0) {
    imputed_values_current$variable_name <- colnames(S)[imputed_values_current$col]
    imputed_values_list <- split(imputed_values_current, imputed_values_current$variable_name)
  } else {
    imputed_values_list <- list()
  }
  
  # Export necessary variables to the cluster
  clusterExport(cl, c("prepare_data_and_cluster_function", "get_data_type", 
                      "fit_cluster_model", "compute_bic_reg",
                      "strategy", "imputed_values_list", "S"), envir = environment())
  
  criterion <- 1
  iter <- 0
  
  while((criterion == 1) & (iter < itermax) & (ncol(S) > 1)) {
    iter <- iter + 1
    if(verbose) cat(paste("iter", iter, "\n"))
    
    # Removing step - Parallel processing
    if(verbose) cat("- removing step\n")
    clusterExport(cl, c("S", "imputed_values_list"), envir = environment())
    remove_results <- parLapply(cl, 1:ncol(S), function(i) {
      S_minus_i <- S[, -i, drop = FALSE]
      model_minus_i <- fit_cluster_model(S_minus_i, G)
      
      # Impute missing values for S_minus_i using model_minus_i
      imputed_values_minus_i <- as.data.frame(missingValues(model_minus_i$model))
      if (!is.null(imputed_values_minus_i) && nrow(imputed_values_minus_i) > 0) {
        imputed_values_minus_i$variable_name <- colnames(S_minus_i)[imputed_values_minus_i$col]
        imputed_values_minus_i$col <- match(imputed_values_minus_i$variable_name, colnames(S_minus_i))
        # Fill in missing values in S_minus_i
        for (j in seq_len(nrow(imputed_values_minus_i))) {
          row_idx <- imputed_values_minus_i$row[j]
          col_idx <- imputed_values_minus_i$col[j]
          value <- imputed_values_minus_i$value[j]
          S_minus_i[row_idx, col_idx] <- value
        }
      }
      
      # Get variable name of S_i
      variable_name <- colnames(S)[i]
      
      # Get imputed values for S_i from imputed_values_list
      imputed_values_S_i <- imputed_values_list[[variable_name]]
      S_i_imputed <- S[, i]
      if (!is.null(imputed_values_S_i) && nrow(imputed_values_S_i) > 0) {
        for (j in seq_len(nrow(imputed_values_S_i))) {
          row_idx <- imputed_values_S_i$row[j]
          value <- imputed_values_S_i$value[j]
          S_i_imputed[row_idx] <- value
        }
      }
      
      # Now x is S_minus_i, y is S_i_imputed
      x <- as.matrix(S_minus_i)
      y <- S_i_imputed
      
      # Now compute BICreg
      BICreg <- compute_bic_reg(x, y, family = get_data_type(y))
      
      list(BIC_total = model_minus_i$BIC + BICreg, 
           BICreg = BICreg,
           model = model_minus_i$model, 
           nbCluster = model_minus_i$nbCluster)
    })
    
    BIC_remove <- sapply(remove_results, function(x) x$BIC_total)
    BIC_reg <- sapply(remove_results, function(x) x$BICreg)
    cdiff_remove <- BICS - BIC_remove
    m_remove <- min(cdiff_remove)
    arg_remove <- which.min(cdiff_remove)
    
    if(m_remove < BIC.diff) {
      # Remove variable
      removed_var <- colnames(S)[arg_remove]
      BICS <- BICS - BIC_reg[arg_remove] - cdiff_remove[arg_remove]
      info <- rbind(info, data.frame(
        Var = removed_var, BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Accepted",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
      NS[[removed_var]] <- S[[removed_var]]
      S <- S[, -arg_remove, drop = FALSE]
      
      # Update current_model
      current_model <- remove_results[[arg_remove]]$model
      
      # Update imputed values
      imputed_values_current <- as.data.frame(missingValues(current_model))
      if (!is.null(imputed_values_current) && nrow(imputed_values_current) > 0) {
        imputed_values_current$variable_name <- colnames(S)[imputed_values_current$col]
        imputed_values_list <- split(imputed_values_current, imputed_values_current$variable_name)
      } else {
        imputed_values_list <- list()
      }
      
    } else {
      info <- rbind(info, data.frame(
        Var = colnames(S)[arg_remove], BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Rejected",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
    }
    
    # Adding step - Parallel processing
    if(ncol(NS) > 2) {
      if(verbose) cat("+ adding step\n")
      clusterExport(cl, c("S", "NS", "imputed_values_list"), envir = environment())
      add_results <- parLapply(cl, 1:ncol(NS), function(i) {
        S_plus_i <- cbind(S, NS[, i, drop = FALSE])
        model_plus_i <- fit_cluster_model(S_plus_i, G)
        
        # Impute missing values for S_plus_i using model_plus_i
        imputed_values_plus_i <- as.data.frame(missingValues(model_plus_i$model))
        if (!is.null(imputed_values_plus_i) && nrow(imputed_values_plus_i) > 0) {
          imputed_values_plus_i$variable_name <- colnames(S_plus_i)[imputed_values_plus_i$col]
          imputed_values_plus_i$col <- match(imputed_values_plus_i$variable_name, colnames(S_plus_i))
          # Fill in missing values in S_plus_i
          for (j in seq_len(nrow(imputed_values_plus_i))) {
            row_idx <- imputed_values_plus_i$row[j]
            col_idx <- imputed_values_plus_i$col[j]
            value <- imputed_values_plus_i$value[j]
            S_plus_i[row_idx, col_idx] <- value
          }
        }
        
        # x is S_plus_i[, 1:ncol(S)]
        x <- as.matrix(S_plus_i[, 1:ncol(S), drop = FALSE])
        # y is S_plus_i[, ncol(S_plus_i)]
        y <- S_plus_i[, ncol(S_plus_i)]
        
        # Now compute BICreg
        BICreg <- compute_bic_reg(x, y, family = get_data_type(y))
        
        list(BIC_total = model_plus_i$BIC,
             BICreg = BICreg,
             model = model_plus_i$model,
             nbCluster = model_plus_i$nbCluster)
      })
      
      BIC_add <- sapply(add_results, function(x) x$BIC_total)
      BIC_reg <- sapply(add_results, function(x) x$BICreg)
      cdiff_add <- BIC_add - (BICS + BIC_reg)
      m_add <- max(cdiff_add)
      arg_add <- which.max(cdiff_add)
      
      if(m_add > BIC.diff) {
        # Add variable
        added_var <- colnames(NS)[arg_add]
        BICS <- BIC_add[arg_add]
        info <- rbind(info, data.frame(
          Var = added_var, BIC = BICS, BICdiff = m_add,
          Step = "Add", Decision = "Accepted",
          Model = class(add_results[[arg_add]]$model)[1],
          G = add_results[[arg_add]]$nbCluster,
          stringsAsFactors = FALSE
        ))
        S[[added_var]] <- NS[[added_var]]
        NS <- NS[, -arg_add, drop = FALSE]
        
        # Update current_model
        current_model <- add_results[[arg_add]]$model
        
        # Update imputed values
        imputed_values_current <- as.data.frame(missingValues(current_model))
        if (!is.null(imputed_values_current) && nrow(imputed_values_current) > 0) {
          imputed_values_current$variable_name <- colnames(S)[imputed_values_current$col]
          imputed_values_list <- split(imputed_values_current, imputed_values_current$variable_name)
        } else {
          imputed_values_list <- list()
        }
        
      } else {
        info <- rbind(info, data.frame(
          Var = colnames(NS)[arg_add], BIC = BIC_add[arg_add], BICdiff = m_add,
          Step = "Add", Decision = "Rejected",
          Model = class(add_results[[arg_add]]$model)[1],
          G = add_results[[arg_add]]$nbCluster,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    if(verbose) {
      print(info[nrow(info), c("Var", "BICdiff", "Step", "Decision")])
    }
    
    # Check if the variables in S have changed
    criterion <- if(ncol(S) == 0) 0 else 1
  }
  
  if(iter >= itermax) 
    warning("Algorithm stopped because maximum number of iterations was reached")
  
  # Prepare output
  varnames <- colnames(X)
  subset <- if(ncol(S) == 0) NULL else match(colnames(S), varnames)
  
  # Fit final model to get optimal number of clusters
  final_model <- fit_cluster_model(S, G)
  optimal_G <- final_model$nbCluster
  
  out <- list(
    variables = varnames,
    subset = subset,
    cluster_model = final_model,
    steps.info = info,
    optimal_G = optimal_G,
    search = "greedy",
    direction = "backward"
  )
  class(out) <- "clustvarsel_mixall"
  return(out)
}

```
 
## Parallel version: SRUW
```{r}
clvarselgrbkw_mixall_surw<- function(X, G = 2:9, strategy = clusterStrategy(),
                                        samp = FALSE, sampsize = 2000,
                                        BIC.diff = 0, itermax = 100,
                                        verbose = interactive(),
                                        num_cores = parallel::detectCores() - 1)
{
  require(parallel)
  
  # Convert X to a data frame if it's not already
  X <- as.data.frame(X)
  n <- nrow(X)
  d <- ncol(X)
  if(is.null(colnames(X))) 
    colnames(X) <- paste("X", 1:d, sep = "")
  G <- setdiff(G, 1)
  
  # If needed, sample the subset of observations
  if(samp) { sub <- sample(1:n, min(sampsize,n), replace = FALSE) }
  else     { sub <- seq.int(1,n) }

  # Function to determine data type
  get_data_type <- function(col) {
    if (is.numeric(col) && all(col %% 1 == 0, na.rm = TRUE)) {
      return("poisson")
    } else if (is.factor(col) || is.character(col)) {
      return("categorical")
    } else {
      return("gaussian")
    }
  }

  # Prepare data and determine clustering function
  prepare_data_and_cluster_function <- function(data) {
    data_types <- sapply(data, get_data_type)
    unique_types <- unique(data_types)
    
    if (length(unique_types) == 1) {
      cluster_fun <- switch(unique_types,
                          "poisson" = clusterPoisson,
                          "categorical" = clusterCategorical,
                          "gaussian" = clusterDiagGaussian)
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data, models = models[[1]], 
                 cluster_fun = cluster_fun, is_mixed = FALSE))
    } else {
      data_list <- lapply(unique_types, function(type) {
        data[, data_types == type, drop = FALSE]
      })
      models <- lapply(unique_types, function(type) {
        switch(type,
               "poisson" = "poisson_pk_ljk",
               "categorical" = "categorical_pk_pjk",
               "gaussian" = "gaussian_pk_sjk")
      })
      return(list(data = data_list, models = models, 
                 cluster_fun = clusterMixedData, 
                 is_mixed = TRUE))
    }
  }

  # Function to fit clustering model
  fit_cluster_model <- function(data, G) {
    tryCatch({
      prepared <- prepare_data_and_cluster_function(data)
      if (prepared$is_mixed) {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                    strategy = strategy, criterion = "BIC", 
                                    models = prepared$models, nbCore = 0)
      } else {
        model <- prepared$cluster_fun(prepared$data, nbCluster = G, 
                                    strategy = strategy, criterion = "BIC",
                                    models = prepared$models, nbCore = 0)
      }
      return(list(model = model, BIC = -model@criterion, nbCluster = model@nbCluster))
    }, error = function(e) {
      return(list(model = NULL, BIC = -Inf, nbCluster = NA))
    })
  }

  # Initialize cluster for parallel processing
  cl <- makeCluster(num_cores)
  on.exit(stopCluster(cl))
  
  # Load required packages on all worker nodes
  clusterEvalQ(cl, {
    library(MixAll)
    library(BMA)
    library(nnet)
    library(mlogitBMA)
    NULL
  })
  
  # Export necessary functions and objects to the cluster
  clusterExport(cl, c("prepare_data_and_cluster_function", "get_data_type", 
                     "fit_cluster_model", "compute_bic_reg",
                     "strategy"), envir = environment())

  # Initialize variables
  S <- X
  U <- data.frame(matrix(ncol = 0, nrow = nrow(X))) 
  W <- data.frame(matrix(ncol = 0, nrow = nrow(X))) 
  NS <- data.frame(matrix(ncol = 0, nrow = nrow(X)))
  info <- data.frame(Var = character(), BIC = numeric(), BICdiff = numeric(), 
                    Step = character(), Decision = character(), 
                    Model = character(), G = integer(), 
                    stringsAsFactors = FALSE)

  # Initial clustering
  if (verbose) cat("Initialize model\n")
  init_model <- fit_cluster_model(S, G)
  BICS <- init_model$BIC

  criterion <- 1
  iter <- 0

  while((criterion == 1) & (iter < itermax) & (ncol(S) > 1)) {
    iter <- iter + 1
    if(verbose) cat(paste("iter", iter, "\n"))

    # Removing step - Parallel processing
    if(verbose) cat("- removing step\n")
    remove_results <- parLapply(cl, 1:ncol(S), function(i) {
      S_minus_i <- S[, -i, drop = FALSE]
      model_minus_i <- fit_cluster_model(S_minus_i, G)
      
      # Calculate BIC for regression of removed variable on remaining variables
      fully_observed <- complete.cases(S_minus_i)
      S_minus_i_obs <- S_minus_i[fully_observed, , drop = FALSE]
      S_i_obs <- S[fully_observed, i, drop = FALSE]
      BICreg <- compute_bic_reg(S_minus_i_obs, S_i_obs,
                               family = get_data_type(S[, i]))
      
      list(BIC_total = model_minus_i$BIC + BICreg, 
           BICreg = BICreg,
           model = model_minus_i$model, 
           nbCluster = model_minus_i$nbCluster)
    })
    
    BIC_remove <- sapply(remove_results, function(x) x$BIC_total)
    BIC_reg <- sapply(remove_results, function(x) x$BICreg)
    cdiff_remove <- BICS - BIC_remove
    m_remove <- min(cdiff_remove)
    arg_remove <- which.min(cdiff_remove)

    if(m_remove < BIC.diff) {
      # Remove variable
      removed_var <- colnames(S)[arg_remove]
      if (BIC_reg[arg_remove] < BICS - cdiff_remove[arg_remove]){
        # Assign to W if the variable is independent of the clustering variables
        W[[removed_var]] <- S[[removed_var]]
      } else {
        # Assign to U if the variable is dependent on a subset of clustering variables
        U[[removed_var]] <- S[[removed_var]]
      }
      BICS <- BICS - BIC_reg[arg_remove] - cdiff_remove[arg_remove]
      info <- rbind(info, data.frame(
        Var = removed_var, BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Accepted",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
      NS[[removed_var]] <- S[[removed_var]]
      S <- S[, -arg_remove, drop = FALSE]
    } else {
      info <- rbind(info, data.frame(
        Var = colnames(S)[arg_remove], BIC = BICS, BICdiff = m_remove,
        Step = "Remove", Decision = "Rejected",
        Model = class(remove_results[[arg_remove]]$model)[1],
        G = remove_results[[arg_remove]]$nbCluster,
        stringsAsFactors = FALSE
      ))
    }

    # Adding step - Parallel processing
      if(ncol(NS) > 2) {
        if(verbose) cat("+ adding step\n")
        add_results <- parLapply(cl, 1:ncol(U), function(i) {
          S_plus_i <- cbind(S, U[, i, drop = FALSE])
          model_plus_i <- fit_cluster_model(S_plus_i, G)
          
          # Calculate BIC for regression of added variable on current variables
          fully_observed <- complete.cases(S)
          S_obs <- S[fully_observed, , drop = FALSE]
          U_i_obs <- U[fully_observed, i, drop = FALSE]
          BICreg <- compute_bic_reg(S_obs, U_i_obs, 
                                   family = get_data_type(S_obs))
          
          list(BIC_total = model_plus_i$BIC,
               BICreg = BICreg,
               model = model_plus_i$model,
               nbCluster = model_plus_i$nbCluster)
        })
  
        BIC_add <- sapply(add_results, function(x) x$BIC_total)
        BIC_reg <- sapply(add_results, function(x) x$BICreg)
        cdiff_add <- BIC_add - (BICS + BIC_reg)
        m_add <- max(cdiff_add)
        arg_add <- which.max(cdiff_add)
  
        if(m_add > BIC.diff) {
          # Add variable to S and update clustering BICS
          added_var <- colnames(U)[arg_add]
          BICS <- BIC_add[arg_add]
          info <- rbind(info, data.frame(
            Var = added_var, BIC = BICS, BICdiff = m_add,
            Step = "Add", Decision = "Accepted",
            Model = class(add_results[[arg_add]]$model)[1],
            G = add_results[[arg_add]]$nbCluster,
            stringsAsFactors = FALSE
          ))
          S[[added_var]] <- U[[added_var]]
          U <- U[, -arg_add, drop = FALSE]
        } else {
          info <- rbind(info, data.frame(
            Var = colnames(U)[arg_add], BIC = BIC_add[arg_add], BICdiff = m_add,
            Step = "Add", Decision = "Rejected",
            Model = class(add_results[[arg_add]]$model)[1],
            G = add_results[[arg_add]]$nbCluster,
            stringsAsFactors = FALSE
          ))
        }
      }

    if(verbose) {
      cat(sprintf("Step %d: S=%d, U=%d, W=%d variables\n", 
              iter, ncol(S), ncol(U), ncol(W)))
      print(info[nrow(info), c("Var", "BICdiff", "Step", "Decision")])
    }

    # Check if the variables in S have changed
    criterion <- if(ncol(S) == 0) 0 else 1
  }

  if(iter >= itermax) 
    warning("Algorithm stopped because maximum number of iterations was reached")

  # Prepare output
  varnames <- colnames(X)
  subset <- if(ncol(S) == 0) NULL else match(colnames(S), varnames)

  # Fit final model to get optimal number of clusters
  final_model <- fit_cluster_model(S, G)
  optimal_G <- final_model$nbCluster

  out <- list(
    variables = varnames,
    subset = subset,
    cluster_model = final_model,
    steps.info = info,
    optimal_G = optimal_G,
    search = "greedy",
    direction = "backward"
  )
  class(out) <- "clustvarsel_mixall"
  return(out)
}
```


# Example: Mclust vs MixAll
```{r}
# Function to create 2D rotation matrix
rotation_matrix_2d <- function(angle) {
  matrix(c(cos(angle), -sin(angle),
           sin(angle), cos(angle)), nrow = 2)
}

# Function to create 3D rotation matrix
rotation_matrix_3d <- function(axis, angle) {
  if (axis == "z") {
    matrix(c(cos(angle), -sin(angle), 0,
             sin(angle), cos(angle), 0,
             0, 0, 1), nrow = 3)
  } else if (axis == "x") {
    matrix(c(1, 0, 0,
             0, cos(angle), -sin(angle),
             0, sin(angle), cos(angle)), nrow = 3)
  }
}

set.seed(123)

# Parameters
n <- 2000
p <- c(0.25, 0.25, 0.2, 0.3)
mu <- rbind(c(0, 0, 0),
            c(-6, 6, 0),
            c(0, 0, 6),
            c(-6, 6, 6))

# Create covariance matrix
A <- rotation_matrix_3d("z", pi/6) %*% rotation_matrix_3d("x", pi/3)
Sigma <- A %*% diag(c(6*sqrt(2), 1, 2)) %*% t(A)
diag_vals <- diag(Sigma)
Sigma <- diag(x = diag_vals)

# Generate mixture data
component <- sample(1:4, n, replace = TRUE, prob = p)
X <- matrix(0, nrow = n, ncol = 3)
for (k in 1:4) {
  idx <- which(component == k)
  X[idx,] <- mvrnorm(length(idx), mu = mu[k,], Sigma = Sigma)
}

# Generate fourth and fifth variables
epsilon <- mvrnorm(n, mu = c(0, 0), Sigma = rotation_matrix_2d(pi/6) %*% diag(c(1, 3)) %*% t(rotation_matrix_2d(pi/6)))
Y45 <- cbind(X[,1:2] %*% matrix(c(0.5, 1, 2, 0), nrow = 2) + epsilon + c(-1, 2))

# Generate two noisy variables
noise <- matrix(rnorm(n*2), ncol = 2)

# Combine all variables
data <- cbind(X, Y45, noise)
colnames(data) <- paste0("V", 1:7)

data_missing <- produce_NA(data, mechanism = "MAR",
                           perc.missing = 0.2)$data.incomp

# MixAll clustering
quick_precise_strategy <- clusterStrategy(
    nbTry = 1,
    nbInit = 5,
    initMethod = "class",
    initAlgo = "SEM",
    nbInitIteration = 25,
    initEpsilon = 1e-4,
    nbShortRun = 5,
    shortRunAlgo = "EM",
    nbShortIteration = 120,
    shortEpsilon = 1e-6,
    longRunAlgo = "EM",
    nbLongIteration = 150,
    longEpsilon = 1e-7
)

thorough_strategy <- clusterStrategy(
    nbTry = 1,
    nbInit = 15,
    initMethod = "class",
    initAlgo = "SEM",
    nbInitIteration = 40,
    initEpsilon = 1e-5,
    nbShortRun = 7,
    shortRunAlgo = "EM",
    nbShortIteration = 200,
    shortEpsilon = 1e-7,
    longRunAlgo = "EM",
    nbLongIteration = 500,
    longEpsilon = 1e-8
)

default_strategy <- clusterStrategy(
    nbTry = 1,
    nbInit = 5,
    initMethod = "random",
    initAlgo = "SEM",
    nbInitIteration = 25,
    initEpsilon = 1e-3,
    nbShortRun = 3,
    shortRunAlgo = "EM",
    nbShortIteration = 100,
    shortEpsilon = 1e-05,
    longRunAlgo = "EM",
    nbLongIteration = 250,
    longEpsilon = 1e-05)

balanced_strategy <- clusterStrategy(
    nbTry = 1,
    nbInit = 10,        
    initMethod = "class", 
    initAlgo = "EM",       
    nbInitIteration = 30,  
    initEpsilon = 1e-4,    
    nbShortRun = 5,   
    shortRunAlgo = "EM",
    nbShortIteration = 150,
    shortEpsilon = 1e-6,  
    longRunAlgo = "EM",
    nbLongIteration = 300, 
    longEpsilon = 1e-7    
)

mixall_model <- clusterDiagGaussian(data = data_missing, nbCluster = 4, 
                                    strategy = quick_precise_strategy,
                                    models=c("gaussian_pk_sjk"))

mixall_model@criterion
imputed_values <- missingValues(mixall_model)
# Function to compute error metrics
compute_error_metrics <- function(actual, predicted) {
  # Mean Squared Error
  mse <- mean((actual - predicted)^2)
  
  # Root Mean Square Error
  rmse <- sqrt(mse)
  
  # Normalized Root Mean Square Error
  nrmse <- rmse / sqrt(mean(actual^2))
  
  return(list(
    mse = mse,
    rmse = rmse,
    nrmse = nrmse
  ))
}

create_imputation_comparison <- function(data, imputed_values) {
  if (!is.data.frame(imputed_values)) {
    imputed_values <- as.data.frame(imputed_values)
  }
  
  # Initialize results dataframe
  comparison_df <- data.frame(
    row = imputed_values$row,
    col = imputed_values$col,
    actual_value = NA,
    imputed_value = imputed_values$value,
    abs_difference = NA
  )
  
  for (i in 1:nrow(comparison_df)) {
    row_idx <- comparison_df$row[i]
    col_idx <- comparison_df$col[i]
    actual_val <- data[row_idx, col_idx]
    comparison_df$actual_value[i] <- actual_val
    comparison_df$abs_difference[i] <- abs(actual_val - comparison_df$imputed_value[i])
  }
  
  error_metrics <- compute_error_metrics(comparison_df$actual_value, comparison_df$imputed_value)
  
  attr(comparison_df, "mse") <- error_metrics$mse
  attr(comparison_df, "rmse") <- error_metrics$rmse
  attr(comparison_df, "nrmse") <- error_metrics$nrmse
  attr(comparison_df, "mean_abs_diff") <- mean(comparison_df$abs_difference)
  attr(comparison_df, "max_abs_diff") <- max(comparison_df$abs_difference)
  
  return(comparison_df)
}

comparison_results <- create_imputation_comparison(data, imputed_values)

print("Imputation Comparison Results:")
print(comparison_results)
print("\nError Metrics:")
print(sprintf("MSE (Mean Squared Error): %f", attr(comparison_results, "mse")))
print(sprintf("RMSE (Root Mean Square Error): %f", attr(comparison_results, "rmse")))
print(sprintf("NRMSE (Normalized RMSE): %f", attr(comparison_results, "nrmse")))
print(sprintf("Mean Absolute Difference: %f", attr(comparison_results, "mean_abs_diff")))
print(sprintf("Maximum Absolute Difference: %f", attr(comparison_results, "max_abs_diff")))

error_summary <- data.frame(
  Metric = c("MSE", "RMSE", "NRMSE", "Mean Abs Diff", "Max Abs Diff"),
  Value = c(
    attr(comparison_results, "mse"),
    attr(comparison_results, "rmse"),
    attr(comparison_results, "nrmse"),
    attr(comparison_results, "mean_abs_diff"),
    attr(comparison_results, "max_abs_diff")
  )
)
print("\nError Metrics Summary:")
print(error_summary)

# Mclust clustering
mclust_model <- Mclust(data, G = 4, modelNames = "VVI")

mclust_model$BIC

# Compare optimal number of clusters
cat("Optimal number of clusters:\n")
cat("MixAll:", mixall_model@nbCluster, "\n")
cat("Mclust:", mclust_model$G, "\n")

# Compare clustering results
mixall_clusters <- mixall_model@ziFit
mclust_clusters <- mclust_model$classification

# Calculate Adjusted Rand Index
# Add 1 to mixall clusters to shift from 0-3 to 1-4 range
mixall_clusters_adjusted <- mixall_clusters + 1

ari <- adjustedRandIndex(mixall_clusters_adjusted, mclust_clusters)
cat("Adjusted Rand Index between MixAll and Mclust clusterings:", ari, "\n")
```

# Example: Variable selection
## Gaussian data (Maugis 5.1)
```{r}
# Function to create 2D rotation matrix
rotation_matrix_2d <- function(angle) {
  matrix(c(cos(angle), -sin(angle),
           sin(angle), cos(angle)), nrow = 2)
}

# Function to create 3D rotation matrix
rotation_matrix_3d <- function(axis, angle) {
  if (axis == "z") {
    matrix(c(cos(angle), -sin(angle), 0,
             sin(angle), cos(angle), 0,
             0, 0, 1), nrow = 3)
  } else if (axis == "x") {
    matrix(c(1, 0, 0,
             0, cos(angle), -sin(angle),
             0, sin(angle), cos(angle)), nrow = 3)
  }
}

set.seed(123)

# Parameters
n <- 2000
p <- c(0.25, 0.25, 0.2, 0.3)
mu <- rbind(c(0, 0, 0),
            c(-6, 6, 0),
            c(0, 0, 6),
            c(-6, 6, 6))

# Create covariance matrix
A <- rotation_matrix_3d("z", pi/6) %*% rotation_matrix_3d("x", pi/3)
Sigma <- A %*% diag(c(6*sqrt(2), 1, 2)) %*% t(A)
diag_vals <- diag(Sigma)
Sigma <- diag(x = diag_vals)

# Generate mixture data
component <- sample(1:4, n, replace = TRUE, prob = p)
X <- matrix(0, nrow = n, ncol = 3)
for (k in 1:4) {
  idx <- which(component == k)
  X[idx,] <- mvrnorm(length(idx), mu = mu[k,], Sigma = Sigma)
}

# Generate fourth and fifth variables
epsilon <- mvrnorm(n, mu = c(0, 0), Sigma = rotation_matrix_2d(pi/6) %*% diag(c(1, 3)) %*% t(rotation_matrix_2d(pi/6)))
Y45 <- cbind(X[,1:2] %*% matrix(c(0.5, 1, 2, 0), nrow = 2) + epsilon + c(-1, 2))

# Generate two noisy variables
noise <- matrix(rnorm(n*2), ncol = 2)

# Combine all variables
data <- cbind(X, Y45, noise)
colnames(data) <- paste0("V", 1:7)
```

## Gaussian data (Maugis 2019)
```{r}
# Set seed for reproducibility
set.seed(123)

# Required library
library(MASS)

# Number of observations and variables
n <- 2000
p <- 14

# Generate cluster labels (1 to 4) equally likely
clusters <- sample(1:4, n, replace = TRUE)

# Define cluster parameters with diagonal covariance structure
# Only V1 and V2 contain clustering information
cluster_params <- list(
  list(
    mean = c(0, 0),
    cov = diag(c(0.5, 0.3))
  ),
  list(
    mean = c(4, 0),
    cov = diag(c(1.5, 0.4))
  ),
  list(
    mean = c(0, 2),
    cov = diag(c(0.4, 1.5))
  ),
  list(
    mean = c(4, 2),
    cov = diag(c(2.0, 1.8))
  )
)

# Initialize matrix for first two variables (RELEVANT for clustering)
y1_2 <- matrix(0, nrow = n, ncol = 2)

# Generate y1 and y2 based on cluster assignments
for (k in 1:4) {
  idx <- which(clusters == k)
  n_k <- length(idx)
  if (n_k > 0) {
    y1_2[idx, ] <- mvrnorm(n_k, 
                          mu = cluster_params[[k]]$mean, 
                          Sigma = cluster_params[[k]]$cov)
  }
}

# Parameters for variables 3 to 11 (IRRELEVANT for clustering)
# These variables depend on V1 and V2 but don't contain additional clustering information
intercepts <- c(0, 0, seq(0.4, 2.8, by = 0.4))
b <- matrix(c(
  0.5,  1.0,    
  2.0,  0.0,
  0.0,  3.0,
  1.0,  2.0,
  2.0,  0.0,
  0.5,  0.0,
  4.0,  0.5,
  3.0,  0.0,
  2.0,  1.0
), nrow = 9, byrow = TRUE)

# Generate epsilon with diagonal covariance
# Large enough variance to mask any potential clustering information
var_scale <- rep(2.0, 9)  # Large variance to ensure no clustering information
Omega <- diag(var_scale)

# Generate epsilon
epsilon <- mvrnorm(n, mu = rep(0, 9), Sigma = Omega)

# Compute y_{3:11} = b * y1_2 + intercepts + epsilon
# These variables depend on y1_2 but don't contain clustering information
predicted <- t(b %*% t(y1_2))
predicted <- sweep(predicted, 2, intercepts, "+")
y3_11 <- predicted + epsilon

# Generate y_{12:14} as pure noise variables (IRRELEVANT and INDEPENDENT)
y12_14 <- matrix(0, nrow = n, ncol = 3)
y12_14[,1] <- rnorm(n, sd = 1.0)
y12_14[,2] <- rnorm(n, sd = 1.0)
y12_14[,3] <- rnorm(n, sd = 1.0)

# Assemble the full dataset
data <- cbind(y1_2, y3_11, y12_14)

# Assign column names
colnames(data) <- paste0("V", 1:14)

# Add true cluster labels as attribute
attr(data, "true_clusters") <- clusters
```

## No Missing values
```{r}
## Run variable selection
result <- clvarselgrbkw_mixall_surw(data, G = 2:6, 
                               strategy = quick_precise_strategy,
                               itermax = 6, verbose = TRUE)

# Print results
print("Variable Selection Results:")
print(result$subset)
print(result$steps.info)

result2 <- clustvarsel(data, G=2:6, search="greedy",
                       direction = "backward", emModels1 = "V", 
                       emModels2 = "VVI", allow.EEE = FALSE, forcetwo = FALSE)

# result3 <- VarSelCluster(data, gvals = 2:8, 
#                          vbleSelec = TRUE, crit.varsel = "BIC", nbcores = 4)

# Evaluate results
true_relevant <- c(1, 2, 3)  # True relevant variables
true_redundant <- c(4, 5)    # True redundant variables
true_noise <- c(6, 7)        # True noise variables

selected_mixall<- result$subset
selected_clustvarsel <- result2$subset
# selected_varsellcm <- match(result3@model@names.relevant, colnames(data))

# Selected variables
print("Selected variables")
print("Clustvarsel: ")
print(selected_clustvarsel)

print("Mixall: ")
print(selected_mixall)

# print("VarSelLCM: ")
# print(selected_varsellcm)

# Correctly identified relevant variables
print("Correctly identified relevant variables")
print("Clustvarsel: ")
print(intersect(selected_clustvarsel, true_relevant))

print("Mixall: ")
print(intersect(selected_mixall, true_relevant))

# print("VarSelLCM: ")
# print(intersect(selected_varsellcm, true_relevant))

# Correctly identified irrelevant variables
print("Correctly identified irrelevant variables (redundant + noise)")
print("Clustvarsel: ")
print(setdiff(1:7, union(selected_clustvarsel, c(true_redundant, true_noise))))

print("Mixall: ")
print(setdiff(1:7, union(selected_mixall, c(true_redundant, true_noise))))

# print("VarSelLCM: ")
# print(setdiff(1:7, union(selected_varsellcm, c(true_redundant, true_noise))))

# Misclassify variables
print("Misclassified variables")
print("Clustvarsel: ")
print(setdiff(union(selected_clustvarsel, true_relevant), 
              intersect(selected_clustvarsel, true_relevant)))

print("Mixall: ")
print(setdiff(union(selected_mixall, true_relevant), 
              intersect(selected_mixall, true_relevant)))
# 
# print("VarSelLCM: ")
# print(setdiff(union(selected_varsellcm, true_relevant), 
#               intersect(selected_varsellcm, true_relevant)))

```

## Missing values
```{r}
# run analysis for a given missing rate
run_missing_analysis <- function(data, missing_rate) {
  set.seed(123 + round(missing_rate * 100))  # Different seeds
  
  data_missing <- produce_NA(data, mechanism = "MAR",
                             perc.missing = missing_rate)$data.incomp
  # Run variable selection methods
  result_mixall <- clvarselgrbkw_mixall_surw(data_missing, G = 2:4, 
                                           strategy = quick_precise_strategy,
                                             itermax = 20, 
                                             verbose = TRUE)

  # Extract results
  selected_mixall <- if(inherits(result_mixall, "try-error")) NULL else result_mixall$subset
  optimal_G <- result_mixall$optimal_G
  cluster_model <- result_mixall$cluster_model
  # Return results
  return(list(
    missing_rate = missing_rate,
    mixall = selected_mixall,
    G = optimal_G,
    cluster_mod = cluster_model
  ))
}


# Run analysis for different missing rates
# missing_rates <- c(0, 0.05, 0.10, 0.15, 0.20)
missing_rates <- c(0.05, 0.10)
results <- lapply(missing_rates, function(rate) run_missing_analysis(data, rate))

# Function to evaluate results
evaluate_results <- function(selected_vars, 
                             true_relevant = c(1, 2, 3),
                           true_redundant = c(4, 5), 
                           true_noise = c(6, 7)) {
  if(is.null(selected_vars)) return(list(
    correctly_identified = numeric(0),
    misclassified = numeric(0)
  ))
  correctly_identified <- intersect(selected_vars, true_relevant)
  misclassified <- setdiff(union(selected_vars, true_relevant),
                          intersect(selected_vars, true_relevant))
  
  return(list(
    correctly_identified = correctly_identified,
    misclassified = misclassified
  ))
}

# Print results for each missing rate
for(i in seq_along(results)) {
  cat(sprintf("\nResults for %d%% missing values:\n", results[[i]]$missing_rate * 100))
  
  # Evaluate each method
  methods <- c("mixall")
  for(method in methods) {
    cat(sprintf("\n%s:\n", method))
    eval_result <- evaluate_results(results[[i]][[method]])
    cat("Num clusters:", 
        paste(results[[i]][['G']], collapse = ", "), "\n")
    cat("Correctly identified relevant variables:", 
        paste(eval_result$correctly_identified, collapse = ", "), "\n")
    cat("Misclassified variables:", 
        paste(eval_result$misclassified, collapse = ", "), "\n")
  }
}
```


