}
}
# Parameters for variables 3 to 11 (IRRELEVANT for clustering)
# These variables depend on V1 and V2 but don't contain additional clustering information
intercepts <- c(0, 0, seq(0.4, 2.8, by = 0.4))
b <- matrix(c(
0.5,  1.0,
2.0,  0.0,
0.0,  3.0,
1.0,  2.0,
2.0,  0.0,
0.5,  0.0,
4.0,  0.5,
3.0,  0.0,
2.0,  1.0
), nrow = 9, byrow = TRUE)
# Generate epsilon with diagonal covariance
# Large enough variance to mask any potential clustering information
var_scale <- rep(2.0, 9)  # Large variance to ensure no clustering information
Omega <- diag(var_scale)
# Generate epsilon
epsilon <- mvrnorm(n, mu = rep(0, 9), Sigma = Omega)
# Compute y_{3:11} = b * y1_2 + intercepts + epsilon
# These variables depend on y1_2 but don't contain clustering information
predicted <- t(b %*% t(y1_2))
predicted <- sweep(predicted, 2, intercepts, "+")
y3_11 <- predicted + epsilon
# Generate y_{12:14} as pure noise variables (IRRELEVANT and INDEPENDENT)
y12_14 <- matrix(0, nrow = n, ncol = 3)
y12_14[,1] <- rnorm(n, sd = 1.0)
y12_14[,2] <- rnorm(n, sd = 1.0)
y12_14[,3] <- rnorm(n, sd = 1.0)
# Assemble the full dataset
data <- cbind(y1_2, y3_11, y12_14)
# Assign column names
colnames(data) <- paste0("V", 1:14)
# Add true cluster labels as attribute
attr(data, "true_clusters") <- clusters
result <- SortvarClust(data, 4)
ldcppmvt <- function(x, mu, SInv, SLogDet) {
log2pi <- log(2.0 * pi)
xdim <- length(x)
constants <- -0.5 * xdim * log2pi
Qf <- as.numeric(t(x - mu) %*% SInv %*% (x - mu))  # Ensure numeric
if (is.nan(Qf) || is.infinite(Qf)) {
return(-Inf)
}
lret <- constants - (0.5 * SLogDet) - (0.5 * Qf)
return(as.numeric(lret))
}
result <- SortvarClust(data, 4)
penalized_EM <- function(X_input, K, lambda, rho, max_iter = 250, tol = 1e-3,
init_P = NULL, full_res = TRUE) {
# X_input: data matrix (n x p)
# K: number of clusters
# lambda: penalty parameter for means
# rho: penalty parameter for precision matrices
# max_iter: maximum number of iterations
# tol: tolerance for convergence
# init_P: optional list of initial parameters
epsilon <- 1e-8  # Small constant to prevent division by zero
n <- nrow(X_input)
p <- ncol(X_input)
# Initialization using InitParameter
if (is.null(init_P)) {
P <- InitParameter(X_input, K, n.start = 250, small.pen = 0.5)
} else {
P <- init_P
}
# Extract initial parameters
X <- P$X  # X is centered
prop <- P$prop
Mu <- P$Mu
CovarianceMatrix <- P$CovarianceMatrix
PrecisionMatrix <- P$PrecisionMatrix
ProbCond <- P$ProbCond
# Initialize penalized log-likelihood
PenLogLik_old <- -Inf
# EM Algorithm
for (itr in 1:max_iter) {
# E-step: Compute conditional probabilities
log_densities <- matrix(0, nrow = n, ncol = K)
for (k in 1:K) {
# Compute log-density for each data point in cluster k
S_k_inv <- PrecisionMatrix[, , k]
S_k_logdet <- -determinant(PrecisionMatrix[, , k], logarithm = TRUE)$modulus
log_densities[, k] <- apply(X, 1, function(x_i) ldcppmvt(x_i, Mu[, k], S_k_inv, S_k_logdet)) + log(pmax(prop[k], epsilon))
}
# Avoid numerical issues by subtracting the max
max_log_densities <- apply(log_densities, 1, max)
log_densities <- sweep(log_densities, 1, max_log_densities, "-")
densities <- exp(log_densities)
densities_sum <- rowSums(densities)
densities_sum[densities_sum == 0] <- epsilon  # Prevent division by zero
ProbCond <- densities / densities_sum  # Conditional probabilities
# M-step: Update parameters
Nk <- colSums(ProbCond)  # Effective number of observations in each cluster
prop <- pmax(Nk / n, epsilon)           # Update class proportions, prevent zero
# Update Means (Mu) with Lasso penalty
for (k in 1:K) {
tau_k <- ProbCond[, k]
Nk_k <- sum(tau_k)
W_k <- PrecisionMatrix[, , k]
Mu_k <- Mu[, k]
for (j in 1:p) {
# Compute Tabs correctly
Tabs <- abs(sum(tau_k * (X %*% W_k[, j])) + Nk_k * Mu_k[j] * W_k[j, j])
if (Tabs <= lambda) {
Mu_k[j] <- 0
} else {
# Ensure proper computation with parentheses
T_num <- sum(tau_k * (X %*% W_k[, j])) - Nk_k * (W_k[j, ] %*% Mu_k) + Nk_k * Mu_k[j] * W_k[j, j]
denominator <- Nk_k * W_k[j, j] + epsilon  # Prevent division by zero
if (T_num < 0) {
Mu_k[j] <- (T_num + lambda) / denominator
} else {
Mu_k[j] <- (T_num - lambda) / denominator
}
}
}
Mu[, k] <- Mu_k
}
# Update covariance matrices with graphical Lasso
for (k in 1:K) {
tau_k <- ProbCond[, k]
Nk_k <- sum(tau_k)
# Compute empirical covariance matrix
diff <- sweep(X, 2, Mu[, k], "-")
weighted_diff <- sqrt(tau_k) * diff
S_k <- crossprod(weighted_diff) / Nk_k
# Ensure S_k is positive definite
if (any(is.na(S_k)) || any(is.infinite(S_k))) {
warning(paste("Cluster", k, ": Covariance matrix computation resulted in invalid values."))
next  # Skip updating this cluster
}
# Apply graphical Lasso
rho_tilde <- (2 * rho) / Nk_k
glasso_res <- tryCatch({
glasso::glasso(S_k, rho = rho_tilde, penalize.diagonal = FALSE, thr = 1e-4, maxit = 1000)
}, error = function(e) {
warning(paste("Graphical Lasso failed for cluster", k, ":", e$message))
return(NULL)
})
if (!is.null(glasso_res)) {
CovarianceMatrix[, , k] <- glasso_res$w
PrecisionMatrix[, , k] <- glasso_res$wi
}
}
# Compute penalized log-likelihood
PenLogLik_new <- 0
for (i in 1:n) {
log_dens_i <- numeric(K)
for (k in 1:K) {
S_k_inv <- PrecisionMatrix[, , k]
S_k_logdet <- -determinant(PrecisionMatrix[, , k], logarithm = TRUE)$modulus
log_dens_i[k] <- ldcppmvt(X[i, ], Mu[, k], S_k_inv, S_k_logdet) + log(pmax(prop[k], epsilon))
}
max_log_dens_i <- max(log_dens_i)
log_dens_i <- log_dens_i - max_log_dens_i
dens_i <- exp(log_dens_i)
PenLogLik_new <- PenLogLik_new + (max_log_dens_i + log(sum(dens_i)))
}
# Subtract penalties
PenLogLik_new <- PenLogLik_new - lambda * sum(abs(Mu)) - rho * sum(abs(PrecisionMatrix))
# Check convergence
if (abs(PenLogLik_new - PenLogLik_old) < tol) {
message("Converged in ", itr, " iterations.")
break
}
PenLogLik_old <- PenLogLik_new
if (itr %% 10 == 0) {
message("Iteration ", itr, " Penalized Log-Likelihood: ", PenLogLik_new)
}
}
# Determine variable roles
Mu_abs_sum <- rowSums(abs(Mu))
variable_roles <- ifelse(Mu_abs_sum == 0, 0, 1)  # 0: variable not used, 1: variable used
if (full_res){
return(list(Mu = Mu, CovarianceMatrix = CovarianceMatrix, PrecisionMatrix = PrecisionMatrix,
ProbCond = ProbCond, prop = prop, PenLogLik = PenLogLik_new, variable_roles = variable_roles))
}
else{
return(variable_roles)
}
}
ldcppmvt <- function(x, mu, SInv, SLogDet) {
log2pi <- log(2.0 * pi)
xdim <- length(x)
constants <- -0.5 * xdim * log2pi
Qf <- as.numeric(t(x - mu) %*% SInv %*% (x - mu))  # Ensure numeric
if (is.nan(Qf) || is.infinite(Qf)) {
return(-Inf)
}
lret <- constants - (0.5 * SLogDet) - (0.5 * Qf)
return(as.numeric(lret))
}
result <- SortvarClust(data, 4)
SortvarClust <- function(x,
nbcluster,
type = "lasso",
lambda = seq(20, 100, by = 10),
rho = seq(1, 2, length = 2),
nbcores = min(2, parallel::detectCores(logical = FALSE)))
{
# Load necessary packages
library(parallel)
# Check 'x' parameter
if (missing(x)) {
stop("The 'x' parameter is missing!")
}
if (!is.matrix(x) && !is.data.frame(x)) {
stop(paste(sQuote("x"), "must be a matrix or data frame"))
}
# Check 'nbcluster' parameter
if (missing(nbcluster)) {
stop("The 'nbcluster' parameter is missing!")
}
if (any(!is.wholenumber(nbcluster))) {
stop("'nbcluster' must contain only integer values!")
}
if (any(nbcluster < 1)) {
stop(paste(sQuote("nbcluster"), "must be integers greater than 0!"))
}
# Check 'lambda' parameter
if (!is.vector(lambda) || length(lambda) <= 1) {
stop(paste(sQuote("lambda"), "must be a vector with length >= 2"))
}
if (any(lambda <= 0)) {
stop("All values in 'lambda' must be greater than 0!")
}
# Check 'rho' parameter
if (!is.vector(rho)) {
stop(paste(sQuote("rho"), "must be a vector"))
}
if (any(rho <= 0)) {
stop("All values in 'rho' must be greater than 0!")
}
# Check 'nbcores' parameter
if (!is.wholenumber(nbcores) || (nbcores < 1)) {
stop(paste(sQuote("nbcores"), "must be an integer greater than 0"))
}
# Scale the data
x <- scale(x)
n <- as.integer(nrow(x))
p <- as.integer(ncol(x))
K <- as.integer(nbcluster)
## Initialize OrderVariable matrix
OrderVariable <- matrix(NA, nrow = length(nbcluster), ncol = p)
if (type == "lasso") {
# Call ClusteringEMGlasso to obtain VarRole
VarRole <- ClusteringEMGlasso(x, nbcluster, lambda, rho, nbcores)
# VarRole has dimensions (number of penalty combinations, p, number of nbcluster options)
# Initialize Matrix0 to store counts of variable selection
Matrix0 <- matrix(0, nrow = length(nbcluster), ncol = p)
for (k in seq_along(nbcluster)) {
# Sum over the penalty combinations for each variable
Matrix0[k, ] <- colSums(VarRole[, , k], na.rm = TRUE)
}
for (k in seq_along(nbcluster)) {
# Order variables based on their total selection counts
# The variables with higher counts are considered more important
OrderVariable[k, ] <- order(Matrix0[k, ], decreasing = TRUE)
}
} else if (type == "likelihood") {
# Placeholder for orderlikC function, which needs to be defined elsewhere
if (!exists("orderlikC")) {
stop("Function 'orderlikC' is not defined. Please define it before using 'type = \"likelihood\"'.")
}
for (k in seq_along(nbcluster)) {
OrderVariable[k, ] <- orderlikC(x, nbcluster[k], nbcores)
}
} else {
stop("Unknown 'type'. Please specify 'lasso' or 'likelihood'.")
}
return(OrderVariable)
}
result <- SortvarClust(data, 4)
ClusteringEMGlasso <- function(data,
nbcluster,
lambda,
rho,
nbcores = 1)
{
# Load required packages
library(glasso)
library(parallel)
library(matrixStats)
library(MASS)
# Ensure data is a matrix
data <- as.matrix(data)
n <- nrow(data)
p <- ncol(data)
# Adjust number of cores if necessary
total_combinations <- length(lambda) * length(rho)
if (total_combinations < nbcores)
nbcores <- total_combinations
# Prepare penalty grid
pen_grid <- expand.grid(lambda = lambda, rho = rho)
n_pen <- nrow(pen_grid)
# Initialize VarRole array
VarRole <- array(0, dim = c(n_pen, p, length(nbcluster)))
# Set up cluster for parallel computation if on Windows
if (Sys.info()["sysname"] == "Windows") {
cl <- makeCluster(nbcores)
clusterEvalQ(cl, {
library(glasso)
library(matrixStats)
library(MASS)
})
}
# Loop over each number of clusters
for (k_idx in seq_along(nbcluster)) {
K <- nbcluster[k_idx]
# Initialize parameters using InitParameter
P <- InitParameter(data, K, n.start = 250, small.pen = 0.5)
# Define function to run penalized_EM for given index
run_penalized_EM <- function(idx) {
params <- pen_grid[idx, ]
lambda_i <- params$lambda
rho_i <- params$rho
result <- tryCatch({
penalized_EM(X = data, K = K, lambda = lambda_i, rho = rho_i, init_P = P)
}, error = function(e) {
NULL
})
return(result)
}
if (Sys.info()["sysname"] == "Windows") {
# Export necessary variables and functions to the cluster
clusterExport(cl, varlist = c("data", "K", "P", "penalized_EM"), envir = environment())
# Run penalized_EM in parallel on the cluster
results_list_K <- parLapply(cl, seq_len(n_pen), run_penalized_EM)
} else {
# Run penalized_EM in parallel using mclapply
results_list_K <- mclapply(seq_len(n_pen), run_penalized_EM, mc.cores = nbcores)
}
# Extract variable roles and store in VarRole array
var_role_matrix <- matrix(0, nrow = n_pen, ncol = p)
for (j in seq_len(n_pen)) {
result <- results_list_K[[j]]
if (!is.null(result) && !is.null(result$variable_roles)) {
var_role_matrix[j, ] <- result$variable_roles
} else {
var_role_matrix[j, ] <- NA  # Use NA for failed computations
}
}
VarRole[, , k_idx] <- var_role_matrix
}
# Stop the cluster if it was created
if (Sys.info()["sysname"] == "Windows") {
stopCluster(cl)
}
# Return the array of variable roles
return(VarRole)
}
SortvarClust <- function(x,
nbcluster,
type = "lasso",
lambda = seq(20, 100, by = 10),
rho = seq(1, 2, length = 2),
nbcores = min(2, parallel::detectCores(logical = FALSE)))
{
# Load necessary packages
library(parallel)
# Check 'x' parameter
if (missing(x)) {
stop("The 'x' parameter is missing!")
}
if (!is.matrix(x) && !is.data.frame(x)) {
stop(paste(sQuote("x"), "must be a matrix or data frame"))
}
# Check 'nbcluster' parameter
if (missing(nbcluster)) {
stop("The 'nbcluster' parameter is missing!")
}
if (any(!is.wholenumber(nbcluster))) {
stop("'nbcluster' must contain only integer values!")
}
if (any(nbcluster < 1)) {
stop(paste(sQuote("nbcluster"), "must be integers greater than 0!"))
}
# Check 'lambda' parameter
if (!is.vector(lambda) || length(lambda) <= 1) {
stop(paste(sQuote("lambda"), "must be a vector with length >= 2"))
}
if (any(lambda <= 0)) {
stop("All values in 'lambda' must be greater than 0!")
}
# Check 'rho' parameter
if (!is.vector(rho)) {
stop(paste(sQuote("rho"), "must be a vector"))
}
if (any(rho <= 0)) {
stop("All values in 'rho' must be greater than 0!")
}
# Check 'nbcores' parameter
if (!is.wholenumber(nbcores) || (nbcores < 1)) {
stop(paste(sQuote("nbcores"), "must be an integer greater than 0"))
}
# Scale the data
x <- scale(x)
n <- as.integer(nrow(x))
p <- as.integer(ncol(x))
K <- as.integer(nbcluster)
## Initialize OrderVariable matrix
OrderVariable <- matrix(NA, nrow = length(nbcluster), ncol = p)
if (type == "lasso") {
# Call ClusteringEMGlasso to obtain VarRole
VarRole <- ClusteringEMGlasso(x, nbcluster, lambda, rho, nbcores)
# VarRole has dimensions (number of penalty combinations, p, number of nbcluster options)
# Initialize Matrix0 to store counts of variable selection
Matrix0 <- matrix(0, nrow = length(nbcluster), ncol = p)
for (k in seq_along(nbcluster)) {
# Sum over the penalty combinations for each variable
Matrix0[k, ] <- colSums(VarRole[, , k], na.rm = TRUE)
}
for (k in seq_along(nbcluster)) {
# Order variables based on their total selection counts
# The variables with higher counts are considered more important
OrderVariable[k, ] <- order(Matrix0[k, ], decreasing = TRUE)
}
} else if (type == "likelihood") {
# Placeholder for orderlikC function, which needs to be defined elsewhere
if (!exists("orderlikC")) {
stop("Function 'orderlikC' is not defined. Please define it before using 'type = \"likelihood\"'.")
}
for (k in seq_along(nbcluster)) {
OrderVariable[k, ] <- orderlikC(x, nbcluster[k], nbcores)
}
} else {
stop("Unknown 'type'. Please specify 'lasso' or 'likelihood'.")
}
return(OrderVariable)
}
is.wholenumber <- function(x, tol = .Machine$double.eps^0.5) abs(x - round(x)) < tol
# Set seed for reproducibility
set.seed(123)
# Number of observations and variables
n <- 2000
p <- 14
# Generate cluster labels (1 to 4) equally likely
clusters <- sample(1:4, n, replace = TRUE)
# Define cluster parameters with diagonal covariance structure
# Only V1 and V2 contain clustering information
cluster_params <- list(
list(
mean = c(0, 0),
cov = diag(c(0.5, 0.3))
),
list(
mean = c(4, 0),
cov = diag(c(1.5, 0.4))
),
list(
mean = c(0, 2),
cov = diag(c(0.4, 1.5))
),
list(
mean = c(4, 2),
cov = diag(c(2.0, 1.8))
)
)
# Initialize matrix for first two variables (RELEVANT for clustering)
y1_2 <- matrix(0, nrow = n, ncol = 2)
# Generate y1 and y2 based on cluster assignments
for (k in 1:4) {
idx <- which(clusters == k)
n_k <- length(idx)
if (n_k > 0) {
y1_2[idx, ] <- mvrnorm(n_k,
mu = cluster_params[[k]]$mean,
Sigma = cluster_params[[k]]$cov)
}
}
# Parameters for variables 3 to 11 (IRRELEVANT for clustering)
# These variables depend on V1 and V2 but don't contain additional clustering information
intercepts <- c(0, 0, seq(0.4, 2.8, by = 0.4))
b <- matrix(c(
0.5,  1.0,
2.0,  0.0,
0.0,  3.0,
1.0,  2.0,
2.0,  0.0,
0.5,  0.0,
4.0,  0.5,
3.0,  0.0,
2.0,  1.0
), nrow = 9, byrow = TRUE)
# Generate epsilon with diagonal covariance
# Large enough variance to mask any potential clustering information
var_scale <- rep(2.0, 9)  # Large variance to ensure no clustering information
Omega <- diag(var_scale)
# Generate epsilon
epsilon <- mvrnorm(n, mu = rep(0, 9), Sigma = Omega)
# Compute y_{3:11} = b * y1_2 + intercepts + epsilon
# These variables depend on y1_2 but don't contain clustering information
predicted <- t(b %*% t(y1_2))
predicted <- sweep(predicted, 2, intercepts, "+")
y3_11 <- predicted + epsilon
# Generate y_{12:14} as pure noise variables (IRRELEVANT and INDEPENDENT)
y12_14 <- matrix(0, nrow = n, ncol = 3)
y12_14[,1] <- rnorm(n, sd = 1.0)
y12_14[,2] <- rnorm(n, sd = 1.0)
y12_14[,3] <- rnorm(n, sd = 1.0)
# Assemble the full dataset
data <- cbind(y1_2, y3_11, y12_14)
# Assign column names
colnames(data) <- paste0("V", 1:14)
# Add true cluster labels as attribute
attr(data, "true_clusters") <- clusters
result <- SortvarClust(data, 4)
result
